{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jgreen@amber (Joe Green)\n",
      "Subject: Re: Weitek P9000 ?\n",
      "Organization: Harris Computer Systems Division\n",
      "Lines: 14\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: amber.ssd.csd.harris.com\n",
      "X-Newsreader: TIN [version 1.1 PL9]\n",
      "\n",
      "Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\n",
      "> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n",
      "> > Anyone know about the Weitek P9000 graphics chip?\n",
      "> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n",
      "> quadrilateral fill command that requires just the four points.\n",
      "\n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      "\n",
      "--\n",
      "Joe Green\t\t\t\tHarris Corporation\n",
      "jgreen@csd.harris.com\t\t\tComputer Systems Division\n",
      "\"The only thing that really scares me is a person with no sense of humor.\"\n",
      "\t\t\t\t\t\t-- Jonathan Winters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 129796)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#converting a collection of text to a matrix of token counts\n",
    "\n",
    "#count_vect = CountVectorizer()\n",
    "\n",
    "#TfidfVectorizer()  - Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "#if I use TfidfVectorizer() and then TfidfTransformer() then performance is higher in both cases by more then 1%\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english')\n",
    "#term frequency count(word)/total words\n",
    "X_train_counts = tfidf_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "#output [n_samples, n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 101322)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF inverse document frequncy - removing common words like 'the' etc.\n",
    "# only if we used CountVectorizer() before we use TfidfTranform() now, in case of TfidfVectorizer(9 skip this step)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building Naive Bayes Classifier - clf\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=.01).fit(X_train_counts, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show top 10 words for each category\n",
    "\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "show_top10(clf, tfidf_vect, twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "        ('vect', TfidfVectorizer(stop_words='english')),\n",
    "        #('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB(alpha=.01)),\n",
    "    ])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8336431226765799"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing performance on test set\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8278889894475222"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.f1_score(twenty_test.target, predicted, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8224907063197026"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#support vector machines\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([\n",
    "        ('vect', TfidfVectorizer(stop_words='english')),\n",
    "        #('tfidf', TfidfTransformer()),\n",
    "        ('clf-svm', SGDClassifier(loss='hinge', alpha=1e-3, n_iter=5, random_state=42)),\n",
    "    ])\n",
    "_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "np.mean(predicted_svm == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning for naive bayse clf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#               'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050733604383949"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning params for svm with GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  #'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__alpha': (1e-2, 1e-3),\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8954392787696659"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf_svm.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vec = StemmedCountVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline(\n",
    "    [\n",
    "        ('vect', stemmed_count_vec),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('mnb', MultinomialNB(fit_prior=False)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8230217737652682"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
    "np.mean(predicted_mnb_stemmed == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_svm_stemmed = Pipeline(\n",
    "    [\n",
    "        ('vect', stemmed_count_vec),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('msvm', SGDClassifier(loss='hinge',alpha=1e-3, n_iter=5, random_state=42)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305894848645778"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_svm_stemmed = text_svm_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_svm_stemmed = text_svm_stemmed.predict(twenty_test.data)\n",
    "np.mean(predicted_svm_stemmed == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.75      0.75       319\n",
      "          1       0.79      0.74      0.77       389\n",
      "          2       0.80      0.72      0.76       394\n",
      "          3       0.70      0.71      0.70       392\n",
      "          4       0.83      0.82      0.83       385\n",
      "          5       0.87      0.83      0.85       395\n",
      "          6       0.87      0.76      0.81       390\n",
      "          7       0.92      0.90      0.91       396\n",
      "          8       0.91      0.97      0.94       398\n",
      "          9       0.87      0.93      0.90       397\n",
      "         10       0.88      0.99      0.93       399\n",
      "         11       0.84      0.96      0.90       396\n",
      "         12       0.82      0.66      0.73       393\n",
      "         13       0.86      0.88      0.87       396\n",
      "         14       0.83      0.96      0.89       394\n",
      "         15       0.77      0.93      0.85       398\n",
      "         16       0.71      0.94      0.81       364\n",
      "         17       0.92      0.93      0.93       376\n",
      "         18       0.86      0.58      0.69       310\n",
      "         19       0.81      0.43      0.56       251\n",
      "\n",
      "avg / total       0.83      0.83      0.83      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report for stemmed svm\n",
    "#The support is the number of occurrences of each class in y_true\n",
    "y_pred = predicted_svm_stemmed\n",
    "y_true = twenty_test.target\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[238   0   0   1   0   1   0   0   2   2   0   3   1  11   6  35   3   5\n",
      "    1  10]\n",
      " [  1 288  16   5   8  21   3   3   1   5   5  12   5   4   7   1   1   2\n",
      "    0   1]\n",
      " [  0  15 283  28  14  13   3   1   4   5   4   8   1   3   7   1   0   2\n",
      "    0   2]\n",
      " [  3  10  25 277  15   4  11   2   4   3   0   6  18   1   8   0   1   2\n",
      "    1   1]\n",
      " [  1   4   4  27 316   2   6   3   1   2   2   2   6   1   4   1   2   0\n",
      "    1   0]\n",
      " [  2  29  16   1   3 326   3   0   1   1   0   1   1   3   6   1   1   0\n",
      "    0   0]\n",
      " [  0   2   1  29  12   0 297  12   6   3   3   1   9   4   2   1   4   1\n",
      "    2   1]\n",
      " [  0   1   0   2   1   0   2 358   7   3   2   1  12   3   1   0   2   1\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   1   4 388   0   1   0   0   3   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   2   0   0 369  23   0   0   0   0   0   1   1\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2 395   1   0   0   0   1   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   1   0   0   3   0 382   0   2   2   1   2   0\n",
      "    2   0]\n",
      " [  5   4   5  19   7   2   8   6   7   8   1  28 260   7  14   5   1   3\n",
      "    3   0]\n",
      " [  3   1   0   4   1   2   2   0   3   6   3   1   4 348   2   4   3   3\n",
      "    5   1]\n",
      " [  0   4   0   0   1   1   0   0   1   0   1   1   1   2 378   1   1   0\n",
      "    2   0]\n",
      " [  9   0   1   1   0   0   0   0   1   1   1   0   0   3   3 371   0   0\n",
      "    0   7]\n",
      " [  1   1   0   0   0   0   1   1   1   2   1   5   0   1   2   0 343   1\n",
      "    3   1]\n",
      " [  8   3   0   0   0   3   0   0   0   1   4   1   0   1   0   0   1 350\n",
      "    4   0]\n",
      " [  4   1   0   0   0   0   1   0   1   1   4   4   0   3   8   2  96   3\n",
      "  180   2]\n",
      " [ 39   0   1   1   0   0   2   1   0   5   0   0   0   5   5  54  19   5\n",
      "    5 109]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for stemmed svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cm_stemmed_svm.txt', 'w') as f:\n",
    "          f.write(np.array2string(confusion_matrix(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.76      0.78       319\n",
      "          1       0.79      0.73      0.76       389\n",
      "          2       0.81      0.67      0.73       394\n",
      "          3       0.67      0.78      0.72       392\n",
      "          4       0.84      0.83      0.84       385\n",
      "          5       0.87      0.81      0.84       395\n",
      "          6       0.86      0.68      0.76       390\n",
      "          7       0.89      0.91      0.90       396\n",
      "          8       0.92      0.97      0.94       398\n",
      "          9       0.91      0.93      0.92       397\n",
      "         10       0.90      0.98      0.94       399\n",
      "         11       0.77      0.96      0.85       396\n",
      "         12       0.85      0.68      0.75       393\n",
      "         13       0.90      0.79      0.84       396\n",
      "         14       0.84      0.94      0.88       394\n",
      "         15       0.69      0.96      0.80       398\n",
      "         16       0.68      0.94      0.79       364\n",
      "         17       0.93      0.96      0.94       376\n",
      "         18       0.86      0.56      0.68       310\n",
      "         19       0.91      0.39      0.54       251\n",
      "\n",
      "avg / total       0.83      0.82      0.82      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report for stemmed mnb\n",
    "y_pred = predicted_mnb_stemmed\n",
    "y_true = twenty_test.target\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[242   0   0   1   0   1   0   0   0   1   0   2   0   6   5  46   4   7\n",
      "    1   3]\n",
      " [  1 284  13  14   8  17   5   1   0   4   3  16   7   0  10   3   2   1\n",
      "    0   0]\n",
      " [  0  21 263  49   5  13   2   2   3   4   2  15   1   1   6   4   2   1\n",
      "    0   0]\n",
      " [  1   8  17 307  16   4   8   1   1   1   1   5  12   0   7   1   1   0\n",
      "    1   0]\n",
      " [  1   2   7  15 321   1  10   8   0   2   2   5   5   0   3   1   2   0\n",
      "    0   0]\n",
      " [  1  29  11   8   6 320   1   0   0   1   0   9   1   1   4   1   2   0\n",
      "    0   0]\n",
      " [  1   2   4  39  13   1 264  14  10   6   7   0   9   6   3   2   5   2\n",
      "    1   1]\n",
      " [  0   2   2   2   0   1   3 362   6   3   2   2   2   1   2   0   4   0\n",
      "    2   0]\n",
      " [  0   0   0   1   0   0   1   7 385   0   0   2   1   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   1   2   1 370  18   1   0   0   0   0   1   2\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   3 393   1   0   0   0   2   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   0   1   1   3   1   1   1 381   1   2   1   0   1   0\n",
      "    1   0]\n",
      " [  3   5   3  18   8   0   7   6   5   2   2  36 266   6  12   6   2   3\n",
      "    3   0]\n",
      " [  2   2   1   3   1   3   2   0   3   7   4   8   5 314   4  20   4   3\n",
      "    9   1]\n",
      " [  0   4   0   0   0   3   0   2   1   0   0   2   3   4 370   1   1   2\n",
      "    1   0]\n",
      " [  4   0   1   1   0   0   0   0   1   1   1   0   0   2   2 381   0   0\n",
      "    0   4]\n",
      " [  1   0   0   1   0   0   1   0   1   1   0   5   0   1   1   3 343   1\n",
      "    4   1]\n",
      " [  1   1   0   0   0   1   0   0   0   1   1   1   0   0   0   4   2 362\n",
      "    2   0]\n",
      " [  2   1   0   0   1   0   0   1   0   0   1   6   0   1   8   8 102   5\n",
      "  174   0]\n",
      " [ 43   0   1   0   0   0   0   0   0   0   0   0   0   3   5  69  27   2\n",
      "    4  97]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for stemmed mnb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cm_stemmed_mnb.txt', 'w') as f:\n",
    "          f.write(np.array2string(confusion_matrix(y_true, y_pred), separator=','))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
